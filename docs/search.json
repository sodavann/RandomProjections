[
  {
    "objectID": "posts/Bayesian Persuasion/bn.html",
    "href": "posts/Bayesian Persuasion/bn.html",
    "title": "Bayesian Persuasion",
    "section": "",
    "text": "In this post we explore the connections between standard machine learning methods and bayesian inference. We also show how a neural network can be made fully bayesian.\nThe first section is a refresher on bayesian stats, then we discuss the bayesian interpretation of your usual machine learning models, last we implement a bayesian neural network in jax.\n\n\nA common view of machine learning is as an optimization problem over a neural network’s parameters to minimize a loss function.\nFor example given a NN with parameters \\(\\theta\\) we may aim to find\n\\[\\hat{\\theta} = \\arg\\min_{\\theta} \\Vert{f_{\\theta}-x}\\Vert^2\\]\nIn case of a linear model \\(f\\) this is the familiar setting of maximum likelihood estimation. This carries over to the general non-linear case, taking a probabilist view we are finding \\(\\theta\\) satisfying (under certain assumptions about measurement errors). \\[\\hat{\\theta} = \\arg\\max_{\\theta} \\log{p(x | \\theta )}\\]\nIn plain text this reads “which choice of \\(\\theta\\) would make seeing this data the most likely?” Note, this does not answer the question “which \\(\\theta\\) is the most likely given the data we’ve observed?”\nGenerally we are interested in estimating \\(\\theta\\) based on the data \\(p(\\theta | x)\\), not the other way around. The way forward is Bayes theorem:\n\\[p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)}\\]\nThe denominator \\(p(x)\\) is problematic, how are we supposed to figure out the objective likelihood of the data in the real world? \\(p(x)\\) forms a normalizing constant (w.r.t \\(\\theta\\)) ensuring \\(p( \\theta | x)\\) integrates to 1. Bayes theorem is often reformulated as\n\\[p(\\theta|x) \\propto p(x|\\theta)p(\\theta)\\]\nwhere the right hand side is an unnormalized probability.\nWe have one other issue, what is \\(p(\\theta)\\)? If we knew the parameters we wouldn’t be solving for them! Maybe we do have some hunch about the scale and behaviour of the parameters, what if we just assign \\(\\theta\\) a probability distribution a priori? This distribution is a design choice we have to make and \\(p(\\theta\\)) is called the prior distribution of \\(\\theta\\), prior as in prior beliefs or assumptions.\nNote that the improper prior \\(p(\\theta)=1\\) would make \\(p(\\theta|x) \\propto p(x|\\theta)\\) everywhere (and thus equal?). This is often a valid choice even though \\(p(\\theta)=1\\) is not a distribution: it doesn’t integrate to 1, it’s not even integrable. The full right hand side can be integrable though which acts as a theoretical saving grace.\nLet’s start by implementing our own bayesian linear regression. In the following sections we’ll work with log probabilities\n\\[\\log{p(\\theta|x)} \\propto \\log{p(x|\\theta)} + \\log{p(\\theta)}\\]\n\nimport numpy as np\nnp.random.seed(123)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme()\nsns.set_style(\"ticks\")\nsns.color_palette(\"dark\")\nsns.set_context(\"poster\")\nplt.rc(\"axes.spines\", top=False, right=False)\nplt.rc(\"figure\", figsize= (12,8))\n\n# generate synthetic data\nt = np.arange(0,10, 0.1)\nx = t**0.5 + np.random.randn(len(t))*0.3\nsns.scatterplot(x=t, y=x);\nplt.title(\"Generated data\");\nplt.xlabel(\"t\");\nplt.ylabel(\"x\");\n\n\n\n\n\n\n\n\nWe have generated data that is not inherently linear, even though the errors are i.i.d normal, let’s see how this goes.\nOur model will be very simple, a linear model in 1 parameter without intercept.\nWe’ll use a \\(N(0,1)\\) prior on the regression parameter \\(\\theta\\), encoding an assumption that it is relatively small. It’s a weakly informative prior that doesn’t nudge the model in any particular direction. For example if we suspect \\(\\theta&gt;0\\) we could have let the choice of prior reflect this prior belief.\nWe will also assume the likelihood \\(p(x(t)| \\theta) \\sim N(\\theta t,1)\\) according to our linear assumptions.\nNext we want to define the log probabilities of \\(p(x | \\theta)\\) and \\(p(\\theta)\\): our likelihood and prior respectively.\n\ndef lm(theta, t):\n    # our linear model\n    return theta*t\n\ndef logprob_norm(data, mu=0, sigma=1):\n    # log of the normal pdf\n    return -np.log(sigma) - np.log(2*np.pi)/2 - (data - mu)**2 / (2 * sigma**2)\n\ndef logprob_prior(theta, mu=0, sigma=1):\n    return logprob_norm(theta, mu, sigma)\n\ndef logprob_likelihood(x, theta, t,  sigma=1):\n    return logprob_norm(x, lm(theta, t), sigma)\n\nNow how do we sample from the posterior \\(p(\\theta | x)\\) given these unnormalized log probabilities? There are many sophisticated ways to do this including HMC and SVI, but here we’ll use the Gumbel max trick.\n\nN_samples=10000\nlog_posteriors = []\nthetas = []\n\n\n# for loop for relative readability\nfor _ in range(N_samples):\n    # sample from theta prior\n    theta_prior = np.random.normal()\n    thetas.append(theta_prior)\n\n    # log probabilities of prior and likelihood\n    lpp = logprob_prior(theta_prior)\n    lpl = np.sum(logprob_likelihood(x, theta_prior, t))\n    \n    # log probability of posterior.\n    lppost = lpp + lpl\n    log_posteriors.append(lppost)\n\nlog_posteriors=np.array(log_posteriors)\nposterior_samples=[]\n\nfor _ in range(N_samples):\n    # use standard gumbel variables to sample from theta based on the log probs\n    gumbels=np.random.gumbel(0,1,N_samples)\n    theta_ind=np.argmax(log_posteriors + gumbels)\n    posterior_samples.append(thetas[theta_ind])\n\n\nsns.histplot(posterior_samples, bins=30, stat= \"probability\");\nplt.title(r\"posterior $p(\\theta$|x)\");\nplt.xlabel(r\"$\\theta$\");\n\n\n\n\n\n\n\n\n\nmu=np.mean(posterior_samples)\nstd=np.std(posterior_samples)\nmu,std\n\n(0.38109701571399685, 0.017641487193737412)\n\n\n\nsns.scatterplot(x=t, y=x);\nsns.lineplot(x=t, y=mu*t);\nsns.lineplot(x=t, y=(mu-std)*t, color='k', alpha=0.3);\nsns.lineplot(x=t, y=(mu+std)*t, color='k', alpha=0.3);\nplt.title(r\"$\\theta$ mean ± std\");\nplt.xlabel(\"t\");\nplt.ylabel(\"x\");\n\n\n\n\n\n\n\n\nWe have successfully sampled from the posterior distribution \\(p(\\theta|x)\\) and found that \\(\\theta \\approx 0.38\\) with quite some certainty.\nWhat about the predicted values \\(\\tilde{x}(t)\\) given this model? This is also probabilistic given the posterior distribution of \\(\\theta\\). These values are distributed according to the posterior predictive distribution \\[p(\\tilde{x}(t)|x) = \\int p(\\tilde{x}(t)|\\theta ) p(\\theta|x(t)) d\\theta\\]\nWe’re essentially looking for the probability of a new \\(\\tilde{x}\\) given all our data x by averaging over \\(\\theta\\) from the posterior.\nFor example \\(\\tilde{x}|t=4\\)\n\nposterior_predictive_samples=[lm(theta, 4) for theta in posterior_samples]\nsns.histplot(posterior_predictive_samples, bins=30, stat=\"probability\");\nplt.title(r\"posterior predictive $p(\\tilde{x}(t=4)|x)$\");\nplt.xlabel(r\"$\\tilde{x}(t=4)$\");\n\n\n\n\n\n\n\n\nThe square root of 4 is actually 2, maybe it’s time to improve the model?\n\n\n\nLet’s explore the NN mentioned in the beginning, we define a model in Flax NNX and minimize the MSE.\n\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom jax.random import PRNGKey\nimport optax\n\nimport pandas as pd\n\n\nclass NN(nnx.Module):\n    def __init__(self, dim_in=1, dim_out=1,  n_hidden = 3, act = nnx.sigmoid, *, key):\n        self.act = act\n        keys = jax.random.split(key, 3)\n\n        self.layer1 = nnx.Linear(dim_in, n_hidden, rngs = nnx.Rngs(params=keys[0]))\n        self.layer2 = nnx.Linear(n_hidden, n_hidden, rngs = nnx.Rngs(params=keys[1]))\n        self.layer3 = nnx.Linear(n_hidden, dim_out, rngs = nnx.Rngs(params=keys[2]), use_bias=False)\n\n    def __call__(self, x):\n\n        x = self.layer1(x)\n        x = self.act(x)\n        x = self.layer2(x)\n        x = self.act(x)\n        return self.layer3(x)\n    \n    def apply(self, params, x):\n        # will be used later\n        nnx.update(self, params)\n\n        return self.__call__(x)\n\n@nnx.jit\ndef loss_fn(model, t, x):\n    y = model(t)\n    return jnp.mean((x-y)**2)\n\n\nkey=PRNGKey(42)\nmodel = NN(key=key)\n\n\nlr=1e-2\noptimizer = nnx.Optimizer(model, optax.adam(lr))\nmetrics = nnx.MultiMetric(\n  loss=nnx.metrics.Average('loss'),\n)\n\n\n@nnx.jit\ndef train_step(model, optimizer, metrics, t, x):\n  \"\"\"Train for a single step.\"\"\"\n  grad_fn = nnx.value_and_grad(loss_fn)\n  loss, grads = grad_fn(model, t, x)\n  metrics.update(loss=loss)\n  optimizer.update(grads)\n\n\nfor _ in range(1000):\n    train_step(model, optimizer, metrics, jnp.expand_dims(t,-1), jnp.expand_dims(x,-1))\n\n\nx_hat=model(jnp.expand_dims(t,-1))\n\n\nsns.scatterplot(x=t, y=x);\nsns.lineplot(x=t, y=x_hat.reshape(-1));\nplt.title(\"NN predictions\");\nplt.xlabel(\"t\");\nplt.ylabel(r\"$f_{\\theta}(t)$\");\n\n\n\n\n\n\n\n\nOverfitting aside we have found parameters \\(\\theta\\) of the neural network \\(f_{\\theta}\\) that minimize our loss function:\n\\[\\Vert x - f_{\\theta}(t) \\Vert^2\\]\nRemember we generated the data with normal i.i.d noise, in a probabilistic setting it would thus be prudent to model the data generating distribution as\n\\[ x(t) \\sim N(f_{\\theta}(t), \\sigma)\\]\nPerforming a maximum likelihood fit we would maximize the log likelihood \\[\\log {p(x(t) | \\theta)} = \\log{ \\frac{e^{-\\frac{\\Vert x-f_{\\theta}(t) \\Vert^2}{2 \\sigma^2}}}{\\sqrt{2\\pi \\sigma^2}}} \\propto C - \\Vert x - f_{\\theta}(t) \\Vert^2\\]\nFeels familiar? Maximizing the log likelihood is the same as minimizing our MSE loss function.\nOne possible remedy for overfitting is regularization of the neural network parameters, for example \\(L_2\\)-regularization would add a term \\(\\beta \\Vert \\theta \\Vert^2\\) to the loss function, thereby penalizing large weights. This generally has a smoothing effect on the output.\nThe loss function would become \\[\\Vert x - f_{\\theta}(t) \\Vert^2 + \\beta \\Vert \\theta - 0 \\Vert^2\\]\nThe connection to our bayesian linear regression is obvious if we flip the sign, these are the unnormalized negative log probabilities of normal priors and likelihoods. Adding regularization to our loss function turns the (frequentist) maximum likelihood estimate into a (bayesian) maximum a posteriori estimate (MAP).\n\\[\\hat{\\theta} = \\arg\\max_{\\theta} \\log{p(\\theta | x )} = \\arg\\max_{\\theta} \\log{p(x | \\theta )} + \\log{p(\\theta )} = \\arg\\min_{\\theta} \\Vert x - f_{\\theta}(t) \\Vert^2 + \\beta \\Vert \\theta\\Vert^2\\]\nIf you were using \\(L_2\\)-regularization you were a bayesian all along! Whether you call it regularization, shrinkage or a prior the effect is the same: constraining parameter values. Similarly, when you use MSE loss you’re applying a filter on the data, assuming normal i.i.d errors. Probability theory just refuses to go away!\n\\(L_1\\)-regularization? A Laplace prior.\nMAE loss? Laplace likelihood.\n\n\n\nWe learned that the standard neural network has a bayesian interpretation, but something is missing. We didn’t manage to sample from our posterior \\(p(\\theta | x)\\), we just found a point estimate of \\(\\theta\\). In order to be fully bayesian we’d like to average over this posterior distribution when predicting new values.\nDrawing samples from the posterior is a difficult problem in general, the standard way is to sample using MCMC (typically HMC) or SVI. These samplers can be found in a variety of packages and part of excellent PPLs like Numpyro, PYMC and Turing.jl. Here we will try out the Blackjax package and its SVI algorithm pathfinder.\n\ndata=(jnp.expand_dims(t,-1), jnp.expand_dims(x,-1))\n\n\nlogprob_norm_jax = jax.scipy.stats.norm.logpdf\n\nWe’ll use the arbitrary priors \\(p(\\theta_i) \\sim N(0,100)\\) on the neural network parameters. The log likelihood is proportional to \\(1/\\sigma^2\\), in this case roughly equivalent to a \\(L_2\\)-regularization parameter \\(\\beta \\approx\\) 1e-4.\n\n@jax.jit\ndef logprior_fn(params):\n    leaves, _ = jax.tree_util.tree_flatten(params)\n    flat_params = jnp.concatenate([jnp.ravel(a) for a in leaves])\n    return jnp.sum(logprob_norm_jax(flat_params, 0, 100))\n\n@jax.jit\ndef loglikelihood_fn(params, data):\n    t, x = data\n    return jnp.sum(logprob_norm_jax(x, model.apply(params, t), .5 ))\n\n@jax.jit\ndef logdensity_fn(params):\n    return logprior_fn(params) + loglikelihood_fn(params, data)\n\n\nimport blackjax\nnum_warmup = 10000\nnum_samples = 5000\n\n\nrng_key = PRNGKey(0)\nrng_key, rng_subkey = jax.random.split(rng_key)\n\nmodel = NN(act=nnx.sigmoid ,key=rng_key)\nstate = nnx.state(model)\nparam = state.filter(nnx.Param)\n\npathfinder_state, infos = blackjax.vi.pathfinder.approximate(\n    rng_key=rng_subkey,\n    logdensity_fn=logdensity_fn,\n    initial_position=param,\n    num_samples=num_warmup,\n    ftol=1e-12,\n    gtol=1e-12,\n    maxiter=50,\n    maxls=10000\n\n)\n\n# sample from the posterior\nrng_key, rng_subkey = jax.random.split(rng_key)\nposterior_samples, _ = blackjax.vi.pathfinder.sample(\n    rng_key=rng_subkey,\n    state=pathfinder_state,\n    num_samples=num_samples,\n)\n\n\ninfos.path.elbo\n\nArray([-2.8448083e+08,           -inf,           -inf,           -inf,\n                 -inf,           -inf,           -inf,           -inf,\n                 -inf,           -inf,           -inf,           -inf,\n                 -inf,           -inf,           -inf,           -inf,\n                 -inf,           -inf,           -inf,           -inf,\n                 -inf, -1.7234183e+02,           -inf, -1.7494684e+02,\n       -1.6947232e+02, -1.8210046e+02,           -inf,           -inf,\n       -1.7699753e+02,           -inf,           -inf,           -inf,\n                 -inf,           -inf,           -inf, -1.7910097e+02,\n                 -inf,           -inf, -1.8681752e+02, -2.9257407e+02,\n                 -inf, -1.8813480e+02,           -inf,           -inf,\n       -2.0863950e+02,           -inf, -1.9126106e+02, -1.8669675e+02,\n                 -inf,           -inf,           -inf], dtype=float32)\n\n\nOuch! This did not converge well, maybe this algorithm isn’t suitable for our use case, maybe the geometry of the posterior is difficult? There are plenty of other samplers to try for those so inclined.\nLet’s fall back on NUTS, a HMC MCMC sampler.\n\nmodel = NN(act=nnx.sigmoid ,key=rng_key)\nstate = nnx.state(model)\nparam = state.filter(nnx.Param)\n\n\nadapt = blackjax.window_adaptation(\n    blackjax.nuts, logdensity_fn, target_acceptance_rate=0.8\n)\nrng_key, warmup_key = jax.random.split(rng_key)\n\n# warm up the model in order to reach equilibrium\n(last_state, parameters), _ = adapt.run(warmup_key, param, num_warmup)\nkernel = blackjax.nuts(logdensity_fn, **parameters).step\n\n\nkeys = jax.random.split(rng_key, num_samples)\ndef step_fn(key, state):\n        return kernel(key, state)\n\n# sample\ns, info = jax.vmap(step_fn,in_axes=(0, None))(keys, last_state)\n\nposterior_params = s.position\n\n\ndef eval_fn(params, t):\n    return model.apply(params, jnp.expand_dims(t,-1))\n\n\n# apply model with weights sampled from the posterior distribution, e.g. draw x from the posterior predictive\nres = jax.vmap(eval_fn, in_axes=(0, None))(posterior_params, t).squeeze()\n\n# predict out of sample\nt_oos=jnp.arange(0,15,.1)\nres_oos = jax.vmap(eval_fn, in_axes=(0, None))(posterior_params, t_oos).squeeze()\n\n\ndf_res=pd.DataFrame(res).T\ndf_oos=pd.DataFrame(res_oos).T\n\n\nmeans=df_res.mean(axis=1)\nstds = df_res.std(axis=1)\n\nsns.scatterplot(x = t, y = x);\nsns.lineplot(x = t, y = means)\nsns.lineplot(x = t, y = means + stds, color=\"k\", alpha=0.3);\nsns.lineplot(x = t, y = means - stds, color=\"k\", alpha=0.3);\nplt.title(r\"BNN mean ± std\");\nplt.xlabel(\"t\");\nplt.ylabel(r\"$f_{\\theta}(t)$\");\n\n\n\n\n\n\n\n\n\nmeans=df_oos.mean(axis=1)\nstds = df_oos.std(axis=1)\n\nsns.lineplot(x=t_oos, y=np.sqrt(t_oos), alpha=1, color=\"r\");\nplt.plot(t_oos,df_oos.sample(axis=1, n=500), alpha=0.02, color=\"k\");\n\nsns.scatterplot(x=t, y=x, alpha=0.3);\nplt.title(r\"BNN posterior predictive samples\");\nplt.xlabel(\"t\");\nplt.ylabel(r\"$f_{\\theta}(t)$\");\nplt.legend([\"Truth\",\"Samples\"]);\nplt.ylim(0,6)\n\n\n\n\n\n\n\n\nWhen using a BNN our predictions are samples drawn from the posterior predictive distribution, this has the benefit of informing us about the model’s uncertainty. We see that the standard deviation grows when we try to extrapolate. Maybe someday LLMs will have this feature instead of confidently recommending eating rocks."
  },
  {
    "objectID": "posts/Bayesian Persuasion/bn.html#back-to-bayesics",
    "href": "posts/Bayesian Persuasion/bn.html#back-to-bayesics",
    "title": "Bayesian Persuasion",
    "section": "",
    "text": "A common view of machine learning is as an optimization problem over a neural network’s parameters to minimize a loss function.\nFor example given a NN with parameters \\(\\theta\\) we may aim to find\n\\[\\hat{\\theta} = \\arg\\min_{\\theta} \\Vert{f_{\\theta}-x}\\Vert^2\\]\nIn case of a linear model \\(f\\) this is the familiar setting of maximum likelihood estimation. This carries over to the general non-linear case, taking a probabilist view we are finding \\(\\theta\\) satisfying (under certain assumptions about measurement errors). \\[\\hat{\\theta} = \\arg\\max_{\\theta} \\log{p(x | \\theta )}\\]\nIn plain text this reads “which choice of \\(\\theta\\) would make seeing this data the most likely?” Note, this does not answer the question “which \\(\\theta\\) is the most likely given the data we’ve observed?”\nGenerally we are interested in estimating \\(\\theta\\) based on the data \\(p(\\theta | x)\\), not the other way around. The way forward is Bayes theorem:\n\\[p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)}\\]\nThe denominator \\(p(x)\\) is problematic, how are we supposed to figure out the objective likelihood of the data in the real world? \\(p(x)\\) forms a normalizing constant (w.r.t \\(\\theta\\)) ensuring \\(p( \\theta | x)\\) integrates to 1. Bayes theorem is often reformulated as\n\\[p(\\theta|x) \\propto p(x|\\theta)p(\\theta)\\]\nwhere the right hand side is an unnormalized probability.\nWe have one other issue, what is \\(p(\\theta)\\)? If we knew the parameters we wouldn’t be solving for them! Maybe we do have some hunch about the scale and behaviour of the parameters, what if we just assign \\(\\theta\\) a probability distribution a priori? This distribution is a design choice we have to make and \\(p(\\theta\\)) is called the prior distribution of \\(\\theta\\), prior as in prior beliefs or assumptions.\nNote that the improper prior \\(p(\\theta)=1\\) would make \\(p(\\theta|x) \\propto p(x|\\theta)\\) everywhere (and thus equal?). This is often a valid choice even though \\(p(\\theta)=1\\) is not a distribution: it doesn’t integrate to 1, it’s not even integrable. The full right hand side can be integrable though which acts as a theoretical saving grace.\nLet’s start by implementing our own bayesian linear regression. In the following sections we’ll work with log probabilities\n\\[\\log{p(\\theta|x)} \\propto \\log{p(x|\\theta)} + \\log{p(\\theta)}\\]\n\nimport numpy as np\nnp.random.seed(123)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme()\nsns.set_style(\"ticks\")\nsns.color_palette(\"dark\")\nsns.set_context(\"poster\")\nplt.rc(\"axes.spines\", top=False, right=False)\nplt.rc(\"figure\", figsize= (12,8))\n\n# generate synthetic data\nt = np.arange(0,10, 0.1)\nx = t**0.5 + np.random.randn(len(t))*0.3\nsns.scatterplot(x=t, y=x);\nplt.title(\"Generated data\");\nplt.xlabel(\"t\");\nplt.ylabel(\"x\");\n\n\n\n\n\n\n\n\nWe have generated data that is not inherently linear, even though the errors are i.i.d normal, let’s see how this goes.\nOur model will be very simple, a linear model in 1 parameter without intercept.\nWe’ll use a \\(N(0,1)\\) prior on the regression parameter \\(\\theta\\), encoding an assumption that it is relatively small. It’s a weakly informative prior that doesn’t nudge the model in any particular direction. For example if we suspect \\(\\theta&gt;0\\) we could have let the choice of prior reflect this prior belief.\nWe will also assume the likelihood \\(p(x(t)| \\theta) \\sim N(\\theta t,1)\\) according to our linear assumptions.\nNext we want to define the log probabilities of \\(p(x | \\theta)\\) and \\(p(\\theta)\\): our likelihood and prior respectively.\n\ndef lm(theta, t):\n    # our linear model\n    return theta*t\n\ndef logprob_norm(data, mu=0, sigma=1):\n    # log of the normal pdf\n    return -np.log(sigma) - np.log(2*np.pi)/2 - (data - mu)**2 / (2 * sigma**2)\n\ndef logprob_prior(theta, mu=0, sigma=1):\n    return logprob_norm(theta, mu, sigma)\n\ndef logprob_likelihood(x, theta, t,  sigma=1):\n    return logprob_norm(x, lm(theta, t), sigma)\n\nNow how do we sample from the posterior \\(p(\\theta | x)\\) given these unnormalized log probabilities? There are many sophisticated ways to do this including HMC and SVI, but here we’ll use the Gumbel max trick.\n\nN_samples=10000\nlog_posteriors = []\nthetas = []\n\n\n# for loop for relative readability\nfor _ in range(N_samples):\n    # sample from theta prior\n    theta_prior = np.random.normal()\n    thetas.append(theta_prior)\n\n    # log probabilities of prior and likelihood\n    lpp = logprob_prior(theta_prior)\n    lpl = np.sum(logprob_likelihood(x, theta_prior, t))\n    \n    # log probability of posterior.\n    lppost = lpp + lpl\n    log_posteriors.append(lppost)\n\nlog_posteriors=np.array(log_posteriors)\nposterior_samples=[]\n\nfor _ in range(N_samples):\n    # use standard gumbel variables to sample from theta based on the log probs\n    gumbels=np.random.gumbel(0,1,N_samples)\n    theta_ind=np.argmax(log_posteriors + gumbels)\n    posterior_samples.append(thetas[theta_ind])\n\n\nsns.histplot(posterior_samples, bins=30, stat= \"probability\");\nplt.title(r\"posterior $p(\\theta$|x)\");\nplt.xlabel(r\"$\\theta$\");\n\n\n\n\n\n\n\n\n\nmu=np.mean(posterior_samples)\nstd=np.std(posterior_samples)\nmu,std\n\n(0.38109701571399685, 0.017641487193737412)\n\n\n\nsns.scatterplot(x=t, y=x);\nsns.lineplot(x=t, y=mu*t);\nsns.lineplot(x=t, y=(mu-std)*t, color='k', alpha=0.3);\nsns.lineplot(x=t, y=(mu+std)*t, color='k', alpha=0.3);\nplt.title(r\"$\\theta$ mean ± std\");\nplt.xlabel(\"t\");\nplt.ylabel(\"x\");\n\n\n\n\n\n\n\n\nWe have successfully sampled from the posterior distribution \\(p(\\theta|x)\\) and found that \\(\\theta \\approx 0.38\\) with quite some certainty.\nWhat about the predicted values \\(\\tilde{x}(t)\\) given this model? This is also probabilistic given the posterior distribution of \\(\\theta\\). These values are distributed according to the posterior predictive distribution \\[p(\\tilde{x}(t)|x) = \\int p(\\tilde{x}(t)|\\theta ) p(\\theta|x(t)) d\\theta\\]\nWe’re essentially looking for the probability of a new \\(\\tilde{x}\\) given all our data x by averaging over \\(\\theta\\) from the posterior.\nFor example \\(\\tilde{x}|t=4\\)\n\nposterior_predictive_samples=[lm(theta, 4) for theta in posterior_samples]\nsns.histplot(posterior_predictive_samples, bins=30, stat=\"probability\");\nplt.title(r\"posterior predictive $p(\\tilde{x}(t=4)|x)$\");\nplt.xlabel(r\"$\\tilde{x}(t=4)$\");\n\n\n\n\n\n\n\n\nThe square root of 4 is actually 2, maybe it’s time to improve the model?"
  },
  {
    "objectID": "posts/Bayesian Persuasion/bn.html#machine-learning-problem",
    "href": "posts/Bayesian Persuasion/bn.html#machine-learning-problem",
    "title": "Bayesian Persuasion",
    "section": "",
    "text": "Let’s explore the NN mentioned in the beginning, we define a model in Flax NNX and minimize the MSE.\n\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom jax.random import PRNGKey\nimport optax\n\nimport pandas as pd\n\n\nclass NN(nnx.Module):\n    def __init__(self, dim_in=1, dim_out=1,  n_hidden = 3, act = nnx.sigmoid, *, key):\n        self.act = act\n        keys = jax.random.split(key, 3)\n\n        self.layer1 = nnx.Linear(dim_in, n_hidden, rngs = nnx.Rngs(params=keys[0]))\n        self.layer2 = nnx.Linear(n_hidden, n_hidden, rngs = nnx.Rngs(params=keys[1]))\n        self.layer3 = nnx.Linear(n_hidden, dim_out, rngs = nnx.Rngs(params=keys[2]), use_bias=False)\n\n    def __call__(self, x):\n\n        x = self.layer1(x)\n        x = self.act(x)\n        x = self.layer2(x)\n        x = self.act(x)\n        return self.layer3(x)\n    \n    def apply(self, params, x):\n        # will be used later\n        nnx.update(self, params)\n\n        return self.__call__(x)\n\n@nnx.jit\ndef loss_fn(model, t, x):\n    y = model(t)\n    return jnp.mean((x-y)**2)\n\n\nkey=PRNGKey(42)\nmodel = NN(key=key)\n\n\nlr=1e-2\noptimizer = nnx.Optimizer(model, optax.adam(lr))\nmetrics = nnx.MultiMetric(\n  loss=nnx.metrics.Average('loss'),\n)\n\n\n@nnx.jit\ndef train_step(model, optimizer, metrics, t, x):\n  \"\"\"Train for a single step.\"\"\"\n  grad_fn = nnx.value_and_grad(loss_fn)\n  loss, grads = grad_fn(model, t, x)\n  metrics.update(loss=loss)\n  optimizer.update(grads)\n\n\nfor _ in range(1000):\n    train_step(model, optimizer, metrics, jnp.expand_dims(t,-1), jnp.expand_dims(x,-1))\n\n\nx_hat=model(jnp.expand_dims(t,-1))\n\n\nsns.scatterplot(x=t, y=x);\nsns.lineplot(x=t, y=x_hat.reshape(-1));\nplt.title(\"NN predictions\");\nplt.xlabel(\"t\");\nplt.ylabel(r\"$f_{\\theta}(t)$\");\n\n\n\n\n\n\n\n\nOverfitting aside we have found parameters \\(\\theta\\) of the neural network \\(f_{\\theta}\\) that minimize our loss function:\n\\[\\Vert x - f_{\\theta}(t) \\Vert^2\\]\nRemember we generated the data with normal i.i.d noise, in a probabilistic setting it would thus be prudent to model the data generating distribution as\n\\[ x(t) \\sim N(f_{\\theta}(t), \\sigma)\\]\nPerforming a maximum likelihood fit we would maximize the log likelihood \\[\\log {p(x(t) | \\theta)} = \\log{ \\frac{e^{-\\frac{\\Vert x-f_{\\theta}(t) \\Vert^2}{2 \\sigma^2}}}{\\sqrt{2\\pi \\sigma^2}}} \\propto C - \\Vert x - f_{\\theta}(t) \\Vert^2\\]\nFeels familiar? Maximizing the log likelihood is the same as minimizing our MSE loss function.\nOne possible remedy for overfitting is regularization of the neural network parameters, for example \\(L_2\\)-regularization would add a term \\(\\beta \\Vert \\theta \\Vert^2\\) to the loss function, thereby penalizing large weights. This generally has a smoothing effect on the output.\nThe loss function would become \\[\\Vert x - f_{\\theta}(t) \\Vert^2 + \\beta \\Vert \\theta - 0 \\Vert^2\\]\nThe connection to our bayesian linear regression is obvious if we flip the sign, these are the unnormalized negative log probabilities of normal priors and likelihoods. Adding regularization to our loss function turns the (frequentist) maximum likelihood estimate into a (bayesian) maximum a posteriori estimate (MAP).\n\\[\\hat{\\theta} = \\arg\\max_{\\theta} \\log{p(\\theta | x )} = \\arg\\max_{\\theta} \\log{p(x | \\theta )} + \\log{p(\\theta )} = \\arg\\min_{\\theta} \\Vert x - f_{\\theta}(t) \\Vert^2 + \\beta \\Vert \\theta\\Vert^2\\]\nIf you were using \\(L_2\\)-regularization you were a bayesian all along! Whether you call it regularization, shrinkage or a prior the effect is the same: constraining parameter values. Similarly, when you use MSE loss you’re applying a filter on the data, assuming normal i.i.d errors. Probability theory just refuses to go away!\n\\(L_1\\)-regularization? A Laplace prior.\nMAE loss? Laplace likelihood."
  },
  {
    "objectID": "posts/Bayesian Persuasion/bn.html#bayesian-neural-network",
    "href": "posts/Bayesian Persuasion/bn.html#bayesian-neural-network",
    "title": "Bayesian Persuasion",
    "section": "",
    "text": "We learned that the standard neural network has a bayesian interpretation, but something is missing. We didn’t manage to sample from our posterior \\(p(\\theta | x)\\), we just found a point estimate of \\(\\theta\\). In order to be fully bayesian we’d like to average over this posterior distribution when predicting new values.\nDrawing samples from the posterior is a difficult problem in general, the standard way is to sample using MCMC (typically HMC) or SVI. These samplers can be found in a variety of packages and part of excellent PPLs like Numpyro, PYMC and Turing.jl. Here we will try out the Blackjax package and its SVI algorithm pathfinder.\n\ndata=(jnp.expand_dims(t,-1), jnp.expand_dims(x,-1))\n\n\nlogprob_norm_jax = jax.scipy.stats.norm.logpdf\n\nWe’ll use the arbitrary priors \\(p(\\theta_i) \\sim N(0,100)\\) on the neural network parameters. The log likelihood is proportional to \\(1/\\sigma^2\\), in this case roughly equivalent to a \\(L_2\\)-regularization parameter \\(\\beta \\approx\\) 1e-4.\n\n@jax.jit\ndef logprior_fn(params):\n    leaves, _ = jax.tree_util.tree_flatten(params)\n    flat_params = jnp.concatenate([jnp.ravel(a) for a in leaves])\n    return jnp.sum(logprob_norm_jax(flat_params, 0, 100))\n\n@jax.jit\ndef loglikelihood_fn(params, data):\n    t, x = data\n    return jnp.sum(logprob_norm_jax(x, model.apply(params, t), .5 ))\n\n@jax.jit\ndef logdensity_fn(params):\n    return logprior_fn(params) + loglikelihood_fn(params, data)\n\n\nimport blackjax\nnum_warmup = 10000\nnum_samples = 5000\n\n\nrng_key = PRNGKey(0)\nrng_key, rng_subkey = jax.random.split(rng_key)\n\nmodel = NN(act=nnx.sigmoid ,key=rng_key)\nstate = nnx.state(model)\nparam = state.filter(nnx.Param)\n\npathfinder_state, infos = blackjax.vi.pathfinder.approximate(\n    rng_key=rng_subkey,\n    logdensity_fn=logdensity_fn,\n    initial_position=param,\n    num_samples=num_warmup,\n    ftol=1e-12,\n    gtol=1e-12,\n    maxiter=50,\n    maxls=10000\n\n)\n\n# sample from the posterior\nrng_key, rng_subkey = jax.random.split(rng_key)\nposterior_samples, _ = blackjax.vi.pathfinder.sample(\n    rng_key=rng_subkey,\n    state=pathfinder_state,\n    num_samples=num_samples,\n)\n\n\ninfos.path.elbo\n\nArray([-2.8448083e+08,           -inf,           -inf,           -inf,\n                 -inf,           -inf,           -inf,           -inf,\n                 -inf,           -inf,           -inf,           -inf,\n                 -inf,           -inf,           -inf,           -inf,\n                 -inf,           -inf,           -inf,           -inf,\n                 -inf, -1.7234183e+02,           -inf, -1.7494684e+02,\n       -1.6947232e+02, -1.8210046e+02,           -inf,           -inf,\n       -1.7699753e+02,           -inf,           -inf,           -inf,\n                 -inf,           -inf,           -inf, -1.7910097e+02,\n                 -inf,           -inf, -1.8681752e+02, -2.9257407e+02,\n                 -inf, -1.8813480e+02,           -inf,           -inf,\n       -2.0863950e+02,           -inf, -1.9126106e+02, -1.8669675e+02,\n                 -inf,           -inf,           -inf], dtype=float32)\n\n\nOuch! This did not converge well, maybe this algorithm isn’t suitable for our use case, maybe the geometry of the posterior is difficult? There are plenty of other samplers to try for those so inclined.\nLet’s fall back on NUTS, a HMC MCMC sampler.\n\nmodel = NN(act=nnx.sigmoid ,key=rng_key)\nstate = nnx.state(model)\nparam = state.filter(nnx.Param)\n\n\nadapt = blackjax.window_adaptation(\n    blackjax.nuts, logdensity_fn, target_acceptance_rate=0.8\n)\nrng_key, warmup_key = jax.random.split(rng_key)\n\n# warm up the model in order to reach equilibrium\n(last_state, parameters), _ = adapt.run(warmup_key, param, num_warmup)\nkernel = blackjax.nuts(logdensity_fn, **parameters).step\n\n\nkeys = jax.random.split(rng_key, num_samples)\ndef step_fn(key, state):\n        return kernel(key, state)\n\n# sample\ns, info = jax.vmap(step_fn,in_axes=(0, None))(keys, last_state)\n\nposterior_params = s.position\n\n\ndef eval_fn(params, t):\n    return model.apply(params, jnp.expand_dims(t,-1))\n\n\n# apply model with weights sampled from the posterior distribution, e.g. draw x from the posterior predictive\nres = jax.vmap(eval_fn, in_axes=(0, None))(posterior_params, t).squeeze()\n\n# predict out of sample\nt_oos=jnp.arange(0,15,.1)\nres_oos = jax.vmap(eval_fn, in_axes=(0, None))(posterior_params, t_oos).squeeze()\n\n\ndf_res=pd.DataFrame(res).T\ndf_oos=pd.DataFrame(res_oos).T\n\n\nmeans=df_res.mean(axis=1)\nstds = df_res.std(axis=1)\n\nsns.scatterplot(x = t, y = x);\nsns.lineplot(x = t, y = means)\nsns.lineplot(x = t, y = means + stds, color=\"k\", alpha=0.3);\nsns.lineplot(x = t, y = means - stds, color=\"k\", alpha=0.3);\nplt.title(r\"BNN mean ± std\");\nplt.xlabel(\"t\");\nplt.ylabel(r\"$f_{\\theta}(t)$\");\n\n\n\n\n\n\n\n\n\nmeans=df_oos.mean(axis=1)\nstds = df_oos.std(axis=1)\n\nsns.lineplot(x=t_oos, y=np.sqrt(t_oos), alpha=1, color=\"r\");\nplt.plot(t_oos,df_oos.sample(axis=1, n=500), alpha=0.02, color=\"k\");\n\nsns.scatterplot(x=t, y=x, alpha=0.3);\nplt.title(r\"BNN posterior predictive samples\");\nplt.xlabel(\"t\");\nplt.ylabel(r\"$f_{\\theta}(t)$\");\nplt.legend([\"Truth\",\"Samples\"]);\nplt.ylim(0,6)\n\n\n\n\n\n\n\n\nWhen using a BNN our predictions are samples drawn from the posterior predictive distribution, this has the benefit of informing us about the model’s uncertainty. We see that the standard deviation grows when we try to extrapolate. Maybe someday LLMs will have this feature instead of confidently recommending eating rocks."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "As the name suggests."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Random Projections",
    "section": "",
    "text": "Bayesian Persuasion\n\n\n\nMachine Learning\n\n\nBayesian Analysis\n\n\nJax\n\n\n\n\n\n\n\nlta\n\n\nSep 20, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]